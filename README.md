# biGRU_Based_Question_Answering_Model

Question Answering is becoming more and more popular in recent years, while statistical and neural network models have been implemented in this NLP task, advanced sequential models like LSTM and attention based network reached the highest performance. This paper reimplemented those state-of-art machine learning architectures, including Attention Sum reader model by (Rudolf Kadlec and Kleindienst, 2016) and Bi-directionally Attention Flow models by (Sordoni et al., 2016) on CNN dataset. The goal is to compare those modelsâ€™ performance on different scale of the dataset and try different vector representation methods. Our baseline model - logistic regression achieved 41% accuracy, while the best deep learning approach achieved 67.2% accuracy using 175,000 training data with option sum reader model.