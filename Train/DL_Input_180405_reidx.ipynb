{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meif/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, re, sys, time, json, codecs, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from inspect import getargspec\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "os.chdir(\"/Users/meif/Desktop/SI 630 NLP/Project/\")\n",
    "\n",
    "from Code.Input_functions import *\n",
    "\n",
    "% matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "EMBEDDING_WORD = 'Embedding/glove.6B/glove.6B.100d.txt'  # 'Embedding/GoogleNews-vectors-negative300.bin'\n",
    "EMBEDDING_CHAR = 'Embedding/char2vecLearned25'\n",
    "TRAIN_DATA_FILE = 'Dataset/cnn/questions/training'\n",
    "VAL_DATA_FILE = 'Dataset/cnn/questions/validation'\n",
    "TEST_DATA_FILE = 'Dataset/cnn/questions/test'\n",
    "\n",
    "N_TRAININGPOINTS = 100000\n",
    "MAX_NUM_WORDS = 20000 # for filtering\n",
    "\n",
    "MAX_SEQUENCE_LENGTH_NEWS = 300 # median ~ 650\n",
    "MAX_SEQUENCE_LENGTH_QUES = 46 # max ~ 46\n",
    "EMBEDDING_DIM_WORD = 100\n",
    "EMBEDDING_DIM_CHAR = 25\n",
    "EMBEDDING_DIM = EMBEDDING_DIM_WORD + EMBEDDING_DIM_CHAR\n",
    "\n",
    "UNK_WORD = \"<UNK_WORD>\"\n",
    "UNK_CHAR = \"^\"\n",
    "UNK_ENTITY = \"<UNK_ENTITY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load_dataset(data_file, name, first=100000000, remove_stopwords=False, stem_words=False, remove_punc=False, keep_period=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "Finished 1000 questions in train\n",
      "Finished 2000 questions in train\n",
      "Finished 3000 questions in train\n",
      "Finished 4000 questions in train\n",
      "Finished 5000 questions in train\n",
      "Finished 6000 questions in train\n",
      "Finished 7000 questions in train\n",
      "Finished 8000 questions in train\n",
      "Finished 9000 questions in train\n",
      "Finished 10000 questions in train\n",
      "Finished 11000 questions in train\n",
      "Finished 12000 questions in train\n",
      "Finished 13000 questions in train\n",
      "Finished 14000 questions in train\n",
      "Finished 15000 questions in train\n",
      "Finished 16000 questions in train\n",
      "Finished 17000 questions in train\n",
      "Finished 18000 questions in train\n",
      "Finished 19000 questions in train\n",
      "Finished 20000 questions in train\n",
      "Finished 21000 questions in train\n",
      "Finished 22000 questions in train\n",
      "Finished 23000 questions in train\n",
      "Finished 24000 questions in train\n",
      "Finished 25000 questions in train\n",
      "Finished 26000 questions in train\n",
      "Finished 27000 questions in train\n",
      "Finished 28000 questions in train\n",
      "Finished 29000 questions in train\n",
      "Finished 30000 questions in train\n",
      "Finished 31000 questions in train\n",
      "Finished 32000 questions in train\n",
      "Finished 33000 questions in train\n",
      "Finished 34000 questions in train\n",
      "Finished 35000 questions in train\n",
      "Finished 36000 questions in train\n",
      "Finished 37000 questions in train\n",
      "Finished 38000 questions in train\n",
      "Finished 39000 questions in train\n",
      "Finished 40000 questions in train\n",
      "Finished 41000 questions in train\n",
      "Finished 42000 questions in train\n",
      "Finished 43000 questions in train\n",
      "Finished 44000 questions in train\n",
      "Finished 45000 questions in train\n",
      "Finished 46000 questions in train\n",
      "Finished 47000 questions in train\n",
      "Finished 48000 questions in train\n",
      "Finished 49000 questions in train\n",
      "Finished 50000 questions in train\n",
      "Finished 51000 questions in train\n",
      "Finished 52000 questions in train\n",
      "Finished 53000 questions in train\n",
      "Finished 54000 questions in train\n",
      "Finished 55000 questions in train\n",
      "Finished 56000 questions in train\n",
      "Finished 57000 questions in train\n",
      "Finished 58000 questions in train\n",
      "Finished 59000 questions in train\n",
      "Finished 60000 questions in train\n",
      "Finished 61000 questions in train\n",
      "Finished 62000 questions in train\n",
      "Finished 63000 questions in train\n",
      "Finished 64000 questions in train\n",
      "Finished 65000 questions in train\n",
      "Finished 66000 questions in train\n",
      "Finished 67000 questions in train\n",
      "Finished 68000 questions in train\n",
      "Finished 69000 questions in train\n",
      "Finished 70000 questions in train\n",
      "Finished 71000 questions in train\n",
      "Finished 72000 questions in train\n",
      "Finished 73000 questions in train\n",
      "Finished 74000 questions in train\n",
      "Finished 75000 questions in train\n",
      "Finished 76000 questions in train\n",
      "Finished 77000 questions in train\n",
      "Finished 78000 questions in train\n",
      "Finished 79000 questions in train\n",
      "Finished 80000 questions in train\n",
      "Finished 81000 questions in train\n",
      "Finished 82000 questions in train\n",
      "Finished 83000 questions in train\n",
      "Finished 84000 questions in train\n",
      "Finished 85000 questions in train\n",
      "Finished 86000 questions in train\n",
      "Finished 87000 questions in train\n",
      "Finished 88000 questions in train\n",
      "Finished 89000 questions in train\n",
      "Finished 90000 questions in train\n",
      "Finished 91000 questions in train\n",
      "Finished 92000 questions in train\n",
      "Finished 93000 questions in train\n",
      "Finished 94000 questions in train\n",
      "Finished 95000 questions in train\n",
      "Finished 96000 questions in train\n",
      "Finished 97000 questions in train\n",
      "Finished 98000 questions in train\n",
      "Finished 99000 questions in train\n",
      "Finished 100000 questions in train\n",
      "Finished 1000 questions in val\n",
      "Finished 2000 questions in val\n",
      "Finished 3000 questions in val\n",
      "Finished 1000 questions in test\n",
      "Finished 2000 questions in test\n",
      "Finished 3000 questions in test\n",
      "Found 100001 questions in trainset\n",
      "Found 3924 questions in valset\n",
      "Found 3198 questions in testset\n",
      "153.79393792152405 sec\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Loading datasets\")\n",
    "'''\n",
    "datasets = {\"news\":[], \"questions\":[], \"answers\":[]}\n",
    "entities = [(news, questions, answer, entities)]\n",
    "'''\n",
    "entities = defaultdict(list)\n",
    "trainsets, entities[\"train\"] = load_dataset(TRAIN_DATA_FILE, \"train\", N_TRAININGPOINTS, remove_stopwords=False, stem_words=False, remove_punc=False)\n",
    "valsets, entities[\"val\"] = load_dataset(VAL_DATA_FILE, \"val\", N_TRAININGPOINTS//10, remove_stopwords=False, stem_words=False, remove_punc=False)\n",
    "testsets, entities[\"test\"] = load_dataset(TEST_DATA_FILE, \"test\", N_TRAININGPOINTS//10, remove_stopwords=False, stem_words=False, remove_punc=False)\n",
    "print(\"Found {} questions in trainset\".format(len(trainsets[\"answers\"]))) # 380298\n",
    "print(\"Found {} questions in valset\".format(len(valsets[\"answers\"]))) # 3924\n",
    "print(\"Found {} questions in testset\".format(len(testsets[\"answers\"]))) # 3198\n",
    "\n",
    "print(\"{} sec\".format(time.time() - start_time)) # 545sec for all, 68sec for 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entity_switch_dict = {\"train\":{}, \"val\":{}, \"test\":{}}\n",
    "for idx, datasets in [(\"train\", trainsets), (\"val\", valsets), (\"test\", testsets)]:\n",
    "    for k in range(len(datasets[\"news\"])):\n",
    "        entity_switch_dict[idx][str(k)] = {}\n",
    "        lst = datasets[\"news\"][k] + \" \" + datasets[\"questions\"][k]\n",
    "        entity_lst = [i for i in lst.split() if i.startswith(\"@entity\")]\n",
    "        count = 0\n",
    "        for ent in entity_lst:\n",
    "            if ent not in entity_switch_dict[idx][str(k)]:\n",
    "                entity_switch_dict[idx][str(k)][ent] = \"@entity{}\".format(count)\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainsets_reidx = copy.deepcopy(trainsets)\n",
    "valsets_reidx = copy.deepcopy(valsets)\n",
    "testsets_reidx = copy.deepcopy(testsets)\n",
    "for idx, datasets, datasets_reidx in [(\"train\", trainsets, trainsets_reidx), (\"val\", valsets, valsets_reidx), (\"test\", testsets, testsets_reidx)]:\n",
    "    for k in range(len(datasets[\"news\"])):\n",
    "        datasets_reidx[\"news\"][k] = \" \".join([i if not i.startswith(\"@entity\") else entity_switch_dict[idx][str(k)][i] for i in datasets[\"news\"][k].split()])\n",
    "        datasets_reidx[\"questions\"][k] = \" \".join([i if not i.startswith(\"@entity\") else entity_switch_dict[idx][str(k)][i] for i in datasets[\"questions\"][k].split()])\n",
    "        datasets_reidx[\"answers\"][k] = entity_switch_dict[idx][str(k)][datasets[\"answers\"][k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.35% of trainsets have answers\n",
      "Total unique tokens in the trainset: 82190\n",
      "Total unique chars in the trainset: 188\n",
      "Median news length: 699.0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(trainsets_reidx['news'])):\n",
    "    if trainsets_reidx[\"answers\"][i] in trainsets_reidx[\"news\"][i].split()[:300]:\n",
    "        count += 1\n",
    "print(\"{0:.2f}% of trainsets have answers\".format(count/len(trainsets_reidx['news'])*100))\n",
    "print(\"Total unique tokens in the trainset: {}\".format(len(Counter([j for i in trainsets_reidx[\"news\"] for j in i.split()]))))\n",
    "print(\"Total unique chars in the trainset: {}\".format(len(Counter([k for i in trainsets_reidx[\"news\"] for j in i.split() for k in j]))))\n",
    "print(\"Median news length: {}\".format(np.median([len(trainsets_reidx[\"news\"][i].split()) for i in range(len(trainsets_reidx['news']))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.21% of valsets have answers\n",
      "Total unique tokens in the valset: 23273\n",
      "Total unique chars in the valset: 77\n",
      "Median news length: 702.5\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(valsets_reidx['news'])):\n",
    "    if valsets_reidx[\"answers\"][i] in valsets_reidx[\"news\"][i].split()[:300]:\n",
    "        count += 1\n",
    "print(\"{0:.2f}% of valsets have answers\".format(count/len(valsets_reidx['news'])*100))\n",
    "print(\"Total unique tokens in the valset: {}\".format(len(Counter([j for i in valsets_reidx[\"news\"] for j in i.split()]))))\n",
    "print(\"Total unique chars in the valset: {}\".format(len(Counter([k for i in valsets_reidx[\"news\"] for j in i.split() for k in j]))))\n",
    "print(\"Median news length: {}\".format(np.median([len(valsets_reidx[\"news\"][i].split()) for i in range(len(valsets_reidx['news']))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.56% of testsets have answers\n",
      "Total unique tokens in the testset: 22271\n",
      "Total unique chars in the testset: 77\n",
      "Median news length: 650.0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(testsets_reidx['news'])):\n",
    "    if testsets_reidx[\"answers\"][i] in testsets_reidx[\"news\"][i].split()[:300]:\n",
    "        count += 1\n",
    "print(\"{0:.2f}% of testsets have answers\".format(count/len(testsets_reidx['news'])*100))\n",
    "print(\"Total unique tokens in the testset: {}\".format(len(Counter([j for i in testsets_reidx[\"news\"] for j in i.split()]))))\n",
    "print(\"Total unique chars in the testset: {}\".format(len(Counter([k for i in testsets_reidx[\"news\"] for j in i.split() for k in j]))))\n",
    "print(\"Median news length: {}\".format(np.median([len(testsets_reidx[\"news\"][i].split()) for i in range(len(testsets_reidx['news']))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_idx_train = []\n",
    "for i in range(len(trainsets_reidx['news'])):\n",
    "    if trainsets_reidx[\"answers\"][i] not in trainsets_reidx[\"news\"][i].split()[:MAX_SEQUENCE_LENGTH_NEWS]:\n",
    "        remove_idx_train.append(i)\n",
    "        \n",
    "remove_idx_val = []\n",
    "for i in range(len(valsets_reidx['news'])):\n",
    "    if valsets_reidx[\"answers\"][i] not in valsets_reidx[\"news\"][i].split()[:MAX_SEQUENCE_LENGTH_NEWS]:\n",
    "        remove_idx_val.append(i)\n",
    "        \n",
    "remove_idx_test = []\n",
    "for i in range(len(testsets_reidx['news'])):\n",
    "    if testsets_reidx[\"answers\"][i] not in testsets_reidx[\"news\"][i].split()[:MAX_SEQUENCE_LENGTH_NEWS]:\n",
    "        remove_idx_test.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainsets_reidx['news'] = [v for i,v in enumerate(trainsets_reidx['news']) if i not in remove_idx_train] \n",
    "trainsets_reidx['questions'] = [v for i,v in enumerate(trainsets_reidx['questions']) if i not in remove_idx_train] \n",
    "trainsets_reidx['answers'] = [v for i,v in enumerate(trainsets_reidx['answers']) if i not in remove_idx_train]\n",
    "entities[\"train\"] = [v for i,v in enumerate(entities[\"train\"]) if i not in remove_idx_train]\n",
    "\n",
    "valsets_reidx['news'] = [v for i,v in enumerate(valsets_reidx['news']) if i not in remove_idx_val] \n",
    "valsets_reidx['questions'] = [v for i,v in enumerate(valsets_reidx['questions']) if i not in remove_idx_val] \n",
    "valsets_reidx['answers'] = [v for i,v in enumerate(valsets_reidx['answers']) if i not in remove_idx_val]\n",
    "entities[\"val\"] = [v for i,v in enumerate(entities[\"val\"]) if i not in remove_idx_val]\n",
    "\n",
    "testsets_reidx['news'] = [v for i,v in enumerate(testsets_reidx['news']) if i not in remove_idx_test] \n",
    "testsets_reidx['questions'] = [v for i,v in enumerate(testsets_reidx['questions']) if i not in remove_idx_test] \n",
    "testsets_reidx['answers'] = [v for i,v in enumerate(testsets_reidx['answers']) if i not in remove_idx_test]\n",
    "entities[\"test\"] = [v for i,v in enumerate(entities[\"test\"]) if i not in remove_idx_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% of trainsets have answers\n",
      "Total unique tokens in the trainset: 77641\n",
      "Total unique chars in the trainset: 180\n",
      "Median news length: 658.0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(trainsets_reidx['news'])):\n",
    "    if trainsets_reidx[\"answers\"][i] in trainsets_reidx[\"news\"][i].split()[:300]:\n",
    "        count += 1\n",
    "print(\"{0:.2f}% of trainsets have answers\".format(count/len(trainsets_reidx['news'])*100))\n",
    "print(\"Total unique tokens in the trainset: {}\".format(len(Counter([j for i in trainsets_reidx[\"news\"] for j in i.split()]))))\n",
    "print(\"Total unique chars in the trainset: {}\".format(len(Counter([k for i in trainsets_reidx[\"news\"] for j in i.split() for k in j]))))\n",
    "print(\"Median news length: {}\".format(np.median([len(trainsets_reidx[\"news\"][i].split()) for i in range(len(trainsets_reidx['news']))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "property experts say @placeholder investment in @entity1 is set to grow\n",
      "( @entity0 ) sophisticated , glamorous and spacious - - when the super - rich go house - hunting they are searching for something special . real estate in @entity1 swankier suburbs can catch a buyers eye . @entity2 , @entity3 and @entity4 have long been the stomping ground of the elite - - and are now welcoming a new wave of @entity5 investors . \" the @entity6 who are coming into @entity1 now are @entity6 who themselves have worked for their money , \" explains @entity7 , a @entity8 - @entity9 wealth manager based in @entity1 . \" they have grown in industry and are actually part of the exciting story of the @entity5 renaissance , \" she continues . \" it bringing to @entity1 the best of the continent . \" these investors are having a considerable impact on @entity1 property market and they mainly come from just six countries : @entity9 , @entity10 , @entity11 , @entity12 , @entity13 and @entity14 . of these , @entity9 are splashing out the most cash when it comes to bricks and mortar in the @entity8 capital - - typically spending between $ 22 and $ 37 million on securing a property , according to luxury property agents @entity15 . their research shows that over the past three years @entity6 have spent over $ 900 million on luxury residential property in @entity1 . \" the new international @entity5 is very well - traveled , \" explains @entity7 . \" educated in the @entity16 , @entity8 and different parts of @entity17 their taste is definitely more modern and clean . \" @entity18 owning a home in post codes like @entity19 or @entity19 - - around the corner from @entity20 - - means more than having a place to lay your head . these buildings are investments which are expected to gain even bigger value in the coming years . high - end auction house @entity21 says that foreign investors see @entity1 as a \" safe haven \" for prime property investments , and ranks the city as the second most important hub for ultra high - net - worth homes . the only spot more important on the planet is @entity22 . for evidence that @entity1 still attracts high - end buyers , look no further than the sale of a penthouse in @entity2 which fetched $ 40 million earlier this year . educated thinking as well as an intelligent investment , many of the @entity5 buyers see these houses as a way of maintaining long standing cultural ties with @entity1 - - and it here they want to send their children to school . @entity23 , @entity24 , @entity25 are all among the list of respected institutions that teach the offspring of wealthy @entity6 . the @entity9 @entity26 in @entity1 calculates that @entity9 nationals now spend over $ 446 million per year on fees , tutoring and accommodation at @entity8 schools and university . \" @entity5 clients are very much driven by the need to educate their children , \" says @entity7 . \" education usually means putting the children on an international stage , and that one reason why this is feeding into the demand for property in @entity1 . \" indeed , education industry experts @entity27 say there were over 17 , 500 @entity9 studying in @entity8 universities in 2012 - - about 1 , 000 more than the 2009 / 10 academic session . and experts are expecting this trend to continue . \" virtually all the transactions are for end use , not rental investment , which indicates that the @entity5 buyer market in @entity1 has significant room for growth , \" says @entity28 , director at @entity15 . \" african buyers or luxury tenants in @entity1 are currently where the @entity29 and @entity30 were five years ago . they have the resources and desire to purchase or rental luxury homes in @entity1 , \" he adds . \" it is going to be the @entity5 century . \" more from @entity31 read this : @entity31 green lean speed machines read this : @entity5 designs rocking art world editor note : @entity32 covers the macro trends impacting the region and also focuses on the continent key industries and corporations\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(testsets_reidx[\"questions\"][0])\n",
    "print(testsets_reidx[\"news\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word-level Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Handle OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getOODict():\n",
    "    texts = trainsets_reidx[\"news\"] + trainsets_reidx[\"questions\"]\n",
    "\n",
    "    # count all words and entities\n",
    "    count_all = Counter([j for i in texts for j in i.split()])\n",
    "    count_entity, count_word = [], []\n",
    "    for i in count_all.keys():\n",
    "        if i.startswith(\"@entity\"):\n",
    "            count_entity.append((i, count_all[i]))\n",
    "        else:\n",
    "            if len(i) == 1 and re.search(r\"[^A-z0-9 \\\"\\'\\.\\?\\{\\}\\(\\)\\[\\]:;!~@#$%&*<>,/+\\-=_]\", i):\n",
    "                pass\n",
    "            else:\n",
    "                count_word.append((i, count_all[i]))\n",
    "\n",
    "    # OOV, OOE, OOC\n",
    "    \n",
    "    notOOE = Counter(dict(count_entity)).most_common(100000000)\n",
    "    notOOE = [i[0] for i in notOOE if i[1] >= 2]\n",
    "\n",
    "    notOOV = Counter(dict(count_word)).most_common(MAX_NUM_WORDS)\n",
    "    notOOV = [i[0] for i in notOOV]\n",
    "\n",
    "    notOOC = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 \\\"'.?{}()[]:;!~@#$%&*<>,/+\\-=_\")\n",
    "        \n",
    "    OODict = {\"notOOV\":notOOV, \"notOOE\":notOOE, \"notOOC\":notOOC}\n",
    "\n",
    "    return OODict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getOOV(datasets, _type):\n",
    "    \n",
    "    # count all words and entities\n",
    "    count_all = Counter([j for i in datasets[_type] for j in i.split()])\n",
    "    count_entity, count_word, OOC = [], [], []\n",
    "    for i in count_all.keys():\n",
    "        if i.startswith(\"@entity\"):\n",
    "            count_entity.append(i)\n",
    "        else:\n",
    "            if len(i) == 1 and re.search(r\"[^A-z0-9 \\\"\\'\\.\\?\\{\\}\\(\\)\\[\\]:;!~@#$%&*<>,/+\\-=_]\", i):\n",
    "                OOC.append(i)\n",
    "            else:\n",
    "                count_word.append(i)\n",
    "    \n",
    "    # OOV, OOE, OOC\n",
    "    OODict = getOODict()\n",
    "    \n",
    "    OOE = set(count_entity) - set(OODict[\"notOOE\"])\n",
    "    OOV = set(count_word) - set(OODict[\"notOOV\"])\n",
    "    OOC = set([i for i in OOC if i != \"|\"])\n",
    "            \n",
    "    return OOV, OOE, OOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def handelOOV(datasets):\n",
    "    \n",
    "    texts = {\"news\":[], \"questions\":[]}\n",
    "    \n",
    "    for _type in [\"news\", \"questions\"]:\n",
    "        print('Replacing {}'.format(_type))    \n",
    "        OOV, OOE, OOC = getOOV(datasets, _type)\n",
    "        \n",
    "        for i in range(len(datasets[_type])//1000+1):\n",
    "#             start_time = time.time()\n",
    "            print(i)\n",
    "            \n",
    "            text = datasets[_type][i*1000:(i+1)*1000]\n",
    "#             text = [re.sub(r\"( {} )\".format(\" | \".join(list(OOV))), \" {} \".format(UNK_WORD), txt) for txt in text]\n",
    "#             text = [re.sub(r\"( {} )\".format(\" | \".join(list(OOC))), \" {} \".format(UNK_CHAR), txt) for txt in text]\n",
    "#             text = [re.sub(r\"( {} )\".format(\" | \".join(list(OOE))), \" {} \".format(UNK_ENTITY), txt) for txt in text]\n",
    "            for txt in text:\n",
    "                txt_lst = txt.split()\n",
    "                for w in range(len(txt_lst)):\n",
    "                    if txt_lst[w] in OOC:\n",
    "                        txt_lst[w] = UNK_CHAR\n",
    "                    elif txt_lst[w] in OOE:\n",
    "                        txt_lst[w] = UNK_ENTITY\n",
    "                    elif txt_lst[w] in OOV:\n",
    "                        txt_lst[w] = UNK_WORD\n",
    "                texts[_type].append(\" \".join(txt_lst))\n",
    "        \n",
    "#             print(\"{} sec\".format(time.time() - start_time)) # 1600sec x 3\n",
    "            \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "handel_OOV = False\n",
    "\n",
    "if handel_OOV:\n",
    "    trainsets_OOV = handelOOV(trainsets_reidx)\n",
    "    valsets_OOV = handelOOV(valsets_reidx)\n",
    "    testsets_OOV = handelOOV(testsets_reidx)\n",
    "\n",
    "    with open(\"Dataset/GRU/{0}_reidx/trainsets_OOV{0}_reidx.npy\".format(N_TRAININGPOINTS), \"w\") as f:\n",
    "        json.dump(trainsets_OOV, f)\n",
    "    with open(\"Dataset/GRU/{0}_reidx/valsets_OOV{0}_reidx.npy\".format(N_TRAININGPOINTS), \"w\") as f:\n",
    "        json.dump(valsets_OOV, f)\n",
    "    with open(\"Dataset/GRU/{0}_reidx/testsets_OOV{0}_reidx.npy\".format(N_TRAININGPOINTS), \"w\") as f:\n",
    "        json.dump(testsets_OOV, f)\n",
    "else:\n",
    "    with open(\"Dataset/GRU/{0}_reidx/trainsets_OOV{0}_reidx.npy\".format(N_TRAININGPOINTS), \"r\") as f:\n",
    "        trainsets_OOV = json.load(f)\n",
    "    with open(\"Dataset/GRU/{0}_reidx/valsets_OOV{0}_reidx.npy\".format(N_TRAININGPOINTS), \"r\") as f:\n",
    "        valsets_OOV = json.load(f)\n",
    "    with open(\"Dataset/GRU/{0}_reidx/testsets_OOV{0}_reidx.npy\".format(N_TRAININGPOINTS), \"r\") as f:\n",
    "        testsets_OOV = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chop into Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Texts to Sequences (Indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming texts to sequences - Word Level\n",
      "Found 20340 unique tokens\n",
      "Median News Length: 659.0\n",
      "Max Question Length: 46\n",
      "119.04841995239258 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print('Transforming texts to sequences - Word Level')\n",
    "\n",
    "tokenizer_word = Tokenizer(filters='', lower=False, split=\" \", char_level=False)\n",
    "tokenizer_word.fit_on_texts(trainsets_OOV[\"news\"] + trainsets_OOV[\"questions\"])\n",
    "\n",
    "trainSeqNews_word = tokenizer_word.texts_to_sequences(trainsets_OOV[\"news\"])\n",
    "trainSeqQues_word = tokenizer_word.texts_to_sequences(trainsets_OOV[\"questions\"])\n",
    "\n",
    "valSeqNews_word = tokenizer_word.texts_to_sequences(valsets_OOV[\"news\"])\n",
    "valSeqQues_word = tokenizer_word.texts_to_sequences(valsets_OOV[\"questions\"])\n",
    "\n",
    "testSeqNews_word = tokenizer_word.texts_to_sequences(testsets_OOV[\"news\"])\n",
    "testSeqQues_word = tokenizer_word.texts_to_sequences(testsets_OOV[\"questions\"])\n",
    "\n",
    "word_counts = tokenizer_word.word_counts\n",
    "word_index = tokenizer_word.word_index\n",
    "print('Found {} unique tokens'.format(len(word_index)))\n",
    "print(\"Median News Length: {}\".format(np.median(np.array([len(i.split()) for i in trainsets_OOV[\"news\"]] + [len(i.split()) for i in valsets_OOV[\"news\"]]))))\n",
    "print(\"Max Question Length: {}\".format(np.max(np.array([len(i.split()) for i in trainsets_OOV[\"questions\"]] + [len(i.split()) for i in valsets_OOV[\"questions\"]]))))\n",
    "\n",
    "print(\"{} sec\".format(time.time() - start_time)) # 60sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Found 400000 word vectors.\n",
      "17.413054943084717 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print('Indexing word vectors')\n",
    "\n",
    "# embeddings_index = KeyedVectors.load_word2vec_format(EMBEDDING_WORD, binary=True) # a word:vec dictionary\n",
    "# print('Found {} word vectors of word2vec'.format(len(embeddings_index.vocab)))\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDING_WORD)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "print(\"{} sec\".format(time.time() - start_time)) # 60sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7162254]]\n",
      "[[0.5718423]]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(embeddings_index[\"news\"].reshape(1,-1), embeddings_index[\"cnn\"].reshape(1,-1)))\n",
    "print(cosine_similarity(embeddings_index[\"news\"].reshape(1,-1), embeddings_index[\"journal\"].reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix - word\n",
      "Null word embeddings: 17\n",
      "Embedding shape: (20341, 100)\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix - word')\n",
    "\n",
    "nb_words = len(word_index) + 1\n",
    "\n",
    "embedding_word_matrix = np.zeros((nb_words, EMBEDDING_DIM_WORD))\n",
    "for word, i in word_index.items():\n",
    "    if word in embeddings_index and word != UNK_WORD:\n",
    "        embedding_word_matrix[i] = embeddings_index[word]\n",
    "    elif word.startswith(\"@entity\"):\n",
    "        num = int(word[7:])\n",
    "        embedding_word_matrix[i] = np.array([0.1] * 100)\n",
    "        if num < 100:\n",
    "            embedding_word_matrix[i][num] = 1\n",
    "        elif num < 200:\n",
    "            embedding_word_matrix[i][99] = 1\n",
    "            embedding_word_matrix[i][num%100] = 1\n",
    "        else:\n",
    "            embedding_word_matrix[i][99] = 1\n",
    "            embedding_word_matrix[i][98] = 1\n",
    "            embedding_word_matrix[i][num%100] = 1\n",
    "print('Null word embeddings: {}'.format(np.sum(np.sum(embedding_word_matrix, axis=1) == 0)))\n",
    "print('Embedding shape: {}'.format(embedding_word_matrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Char-level Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Texts to Sequences (Indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming texts to sequences - Character Level\n",
      "Found 67 unique tokens\n",
      "272.4809920787811 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print('Transforming texts to sequences - Character Level')\n",
    "\n",
    "texts = trainsets_reidx[\"news\"] + trainsets_reidx[\"questions\"]\n",
    "texts = [re.sub(r\"[^A-z0-9 \\\"\\'\\.\\?\\{\\}\\(\\)\\[\\]:;!~@#$%&*<>,/+\\-=_]\", \"^\", text) for text in texts]\n",
    "\n",
    "tokenizer_char = Tokenizer(filters='', lower=False, split=\" \", char_level=True)\n",
    "tokenizer_char.fit_on_texts(texts)\n",
    "\n",
    "trainSeqNews_char = tokenizer_char.texts_to_sequences(trainsets_reidx[\"news\"])\n",
    "trainSeqQues_char = tokenizer_char.texts_to_sequences(trainsets_reidx[\"questions\"])\n",
    "\n",
    "valSeqNews_char = tokenizer_char.texts_to_sequences(valsets_reidx[\"news\"])\n",
    "valSeqQues_char = tokenizer_char.texts_to_sequences(valsets_reidx[\"questions\"])\n",
    "\n",
    "testSeqNews_char = tokenizer_char.texts_to_sequences(testsets_reidx[\"news\"])\n",
    "testSeqQues_char = tokenizer_char.texts_to_sequences(testsets_reidx[\"questions\"])\n",
    "\n",
    "char_index = tokenizer_char.word_index\n",
    "char_counts = tokenizer_char.word_counts\n",
    "print('Found {} unique tokens'.format(len(char_index)))\n",
    "print(\"{} sec\".format(time.time() - start_time)) # 109sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Generate / Load Char Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 64 word vectors of word2vec\n",
      "106.17054200172424 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "chars = [list(re.sub(r\"[^A-z0-9 \\\"\\'\\.\\?\\{\\}\\(\\)\\[\\]:;!~@#$%&*<>,/+\\-=_]\", \"^\", text)) for text in texts]\n",
    "# char2vecLearned = Word2Vec(chars, size=EMBEDDING_DIM_CHAR, min_count=5)\n",
    "# char2vecLearned.save(EMBEDDING_CHAR)\n",
    "char2vecLearned = Word2Vec.load(EMBEDDING_CHAR).wv\n",
    "print('Found {} word vectors of word2vec'.format(len(char2vecLearned.vocab)))\n",
    "\n",
    "print(\"{} sec\".format(time.time() - start_time)) # 201sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8475163]]\n",
      "[[0.833086]]\n",
      "[[0.8597956]]\n",
      "[[-0.1458748]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(char2vecLearned[\"(\"].reshape(1,-1), char2vecLearned[\")\"].reshape(1,-1)))\n",
    "print(cosine_similarity(char2vecLearned[\"?\"].reshape(1,-1), char2vecLearned[\"!\"].reshape(1,-1)))\n",
    "print(cosine_similarity(char2vecLearned[\",\"].reshape(1,-1), char2vecLearned[\".\"].reshape(1,-1)))\n",
    "print(cosine_similarity(char2vecLearned[\"+\"].reshape(1,-1), char2vecLearned[\"-\"].reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix - char\n",
      "Null char embeddings: 5\n",
      "Embedding shape: (68, 25)\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix - char')\n",
    "\n",
    "nb_chars = len(char_index) + 1\n",
    "\n",
    "embedding_char_matrix = np.zeros((nb_chars, EMBEDDING_DIM_CHAR))\n",
    "for char, i in char_index.items():\n",
    "    if char in char2vecLearned.vocab and char != UNK_CHAR:\n",
    "        embedding_char_matrix[i] = char2vecLearned.word_vec(char)\n",
    "print('Null char embeddings: {}'.format(np.sum(np.sum(embedding_char_matrix, axis=1) == 0)))\n",
    "print('Embedding shape: {}'.format(embedding_char_matrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Input and Labels -- Passage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Pad Word Sequences as Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding sequences\n",
      "Shape of news tensor: (87355, 300)\n",
      "Shape of questions tensor: (87355, 46)\n"
     ]
    }
   ],
   "source": [
    "print('Padding sequences')\n",
    "\n",
    "N_train = pad_sequences(trainSeqNews_word, maxlen=MAX_SEQUENCE_LENGTH_NEWS, truncating=\"post\")\n",
    "Q_train = pad_sequences(trainSeqQues_word, maxlen=MAX_SEQUENCE_LENGTH_QUES, truncating=\"post\")\n",
    "print('Shape of news tensor:', N_train.shape)\n",
    "print('Shape of questions tensor:', Q_train.shape)\n",
    "\n",
    "N_val = pad_sequences(valSeqNews_word, maxlen=MAX_SEQUENCE_LENGTH_NEWS, truncating=\"post\")\n",
    "Q_val = pad_sequences(valSeqQues_word, maxlen=MAX_SEQUENCE_LENGTH_QUES, truncating=\"post\")\n",
    "\n",
    "N_test = pad_sequences(testSeqNews_word, maxlen=MAX_SEQUENCE_LENGTH_NEWS, truncating=\"post\")\n",
    "Q_test = pad_sequences(testSeqQues_word, maxlen=MAX_SEQUENCE_LENGTH_QUES, truncating=\"post\")\n",
    "\n",
    "np.save(\"Dataset/GRU/{0}_reidx/N_train{0}_reidx.npy\".format(N_TRAININGPOINTS), N_train)\n",
    "np.save(\"Dataset/GRU/{0}_reidx/Q_train{0}_reidx.npy\".format(N_TRAININGPOINTS), Q_train)\n",
    "np.save(\"Dataset/GRU/{0}_reidx/N_val{0}_reidx.npy\".format(N_TRAININGPOINTS), N_val)\n",
    "np.save(\"Dataset/GRU/{0}_reidx/Q_val{0}_reidx.npy\".format(N_TRAININGPOINTS), Q_val)\n",
    "np.save(\"Dataset/GRU/{0}_reidx/N_test{0}_reidx.npy\".format(N_TRAININGPOINTS), N_test)\n",
    "np.save(\"Dataset/GRU/{0}_reidx/Q_test{0}_reidx.npy\".format(N_TRAININGPOINTS), Q_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Pad Char Sequences as Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1958"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(\" \".join(i.split()[:300])) for i in trainsets_reidx[\"news\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(\" \".join(i.split()[:300])) for i in trainsets_reidx[\"questions\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding sequences\n",
      "Shape of news tensor: (87355, 1960)\n",
      "Shape of questions tensor: (87355, 270)\n"
     ]
    }
   ],
   "source": [
    "print('Padding sequences')\n",
    "\n",
    "N_train_char = pad_sequences(trainSeqNews_char, maxlen=1960, truncating=\"post\")\n",
    "Q_train_char = pad_sequences(trainSeqQues_char, maxlen=270, truncating=\"post\")\n",
    "print('Shape of news tensor:', N_train_char.shape)\n",
    "print('Shape of questions tensor:', Q_train_char.shape)\n",
    "\n",
    "N_val_char = pad_sequences(valSeqNews_char, maxlen=1960, truncating=\"post\")\n",
    "Q_val_char = pad_sequences(valSeqQues_char, maxlen=270, truncating=\"post\")\n",
    "\n",
    "N_test_char = pad_sequences(testSeqNews_char, maxlen=1960, truncating=\"post\")\n",
    "Q_test_char = pad_sequences(testSeqQues_char, maxlen=270, truncating=\"post\")\n",
    "\n",
    "np.save(\"Dataset/GRU/{0}_reidx/N_train_char{0}_reidx.npy\".format(N_TRAININGPOINTS), N_train_char)\n",
    "np.save(\"Dataset/GRU/{0}_reidx/Q_train_char{0}_reidx.npy\".format(N_TRAININGPOINTS), Q_train_char)\n",
    "np.save(\"Dataset/GRU/{0}_reidx/N_val_char{0}_reidx.npy\".format(N_TRAININGPOINTS), N_val_char)\n",
    "np.save(\"Dataset/GRU/{0}_reidx/Q_val_char{0}_reidx.npy\".format(N_TRAININGPOINTS), Q_val_char)\n",
    "np.save(\"Dataset/GRU/{0}_reidx/N_test_char{0}_reidx.npy\".format(N_TRAININGPOINTS), N_test_char)\n",
    "np.save(\"Dataset/GRU/{0}_reidx/Q_test_char{0}_reidx.npy\".format(N_TRAININGPOINTS), Q_test_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Input Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing input embedding\n",
      "Null word embeddings: 4\n",
      "Embedding shape: (20341, 125)\n"
     ]
    }
   ],
   "source": [
    "print('Preparing input embedding')\n",
    "\n",
    "nb_words = len(word_index) + 1\n",
    "sorted_word_index = sorted(word_index.items(), key=lambda x:x[1])\n",
    "\n",
    "embedding_input_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, wordcode in word_index.items():\n",
    "\n",
    "    if word in [UNK_WORD, UNK_CHAR, UNK_ENTITY]:\n",
    "        continue\n",
    "    \n",
    "    # get word embedding\n",
    "    word_level_embedding = embedding_word_matrix[wordcode]\n",
    "\n",
    "    # get char embedding\n",
    "    char_level_embedding =[]\n",
    "    for char in word:\n",
    "        try:\n",
    "            charcode = char_index[char]\n",
    "        except:\n",
    "            charcode = char_index[UNK_CHAR]\n",
    "        char_level_embedding.append(embedding_char_matrix[charcode])\n",
    "    char_level_embedding = np.mean(np.array(char_level_embedding), axis=0)\n",
    "    \n",
    "    # combine word and char embedding\n",
    "    embedding_input_matrix[wordcode] = np.concatenate((word_level_embedding, char_level_embedding)) # (325,)\n",
    "\n",
    "print('Null word embeddings: {}'.format(np.sum(np.sum(embedding_input_matrix, axis=1) == 0)))\n",
    "print('Embedding shape: {}'.format(embedding_input_matrix.shape))\n",
    "np.save(\"Dataset/GRU/{0}_reidx/embedding_input_matrix{0}_reidx.npy\".format(N_TRAININGPOINTS), embedding_input_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9238118]])"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(embedding_input_matrix[word_index[\"@entity0\"]].reshape(1,-1),\n",
    "                  embedding_input_matrix[word_index[\"@entity1\"]].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Output Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[]\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "max_a = []\n",
    "for i in trainsets_reidx[\"news\"] + valsets_reidx[\"news\"] + testsets_reidx[\"news\"]:\n",
    "    a = list(set([int(j[7:]) for j in i.split()[:300] if j.startswith(\"@entity\")]))\n",
    "    max_a += a\n",
    "print(max(max_a))\n",
    "a = {word_index[i]:i for i in word_index if i.startswith(\"@entity\") and int(i[7:]) > 100}\n",
    "print([j for i in N_train for j in i if j in a])\n",
    "print(max([int(i[7:]) for i in trainsets_reidx[\"answers\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found unique entity: 102\n"
     ]
    }
   ],
   "source": [
    "entity_index = [\"@entity{}\".format(i) for i in range(101)] + [UNK_ENTITY]\n",
    "entity_index = {w: index for (index, w) in enumerate(entity_index)}\n",
    "print('Found unique entity: {}'.format(len(entity_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train labels: 87355\n",
      "y_val labels: 3579\n",
      "y_test labels: 2992\n"
     ]
    }
   ],
   "source": [
    "y_train_multi = np.array([entity_index[trainsets_reidx[\"answers\"][i]] for i in range(len(trainsets_reidx[\"answers\"]))])\n",
    "y_val_multi = np.array([entity_index[valsets_reidx[\"answers\"][i]] for i in range(len(valsets_reidx[\"answers\"]))])\n",
    "y_test_multi = np.array([entity_index[testsets_reidx[\"answers\"][i]] for i in range(len(testsets_reidx[\"answers\"]))])\n",
    "y_train_multi = to_categorical(y_train_multi, num_classes=len(entity_index))\n",
    "y_val_multi = to_categorical(y_val_multi, num_classes=len(entity_index))\n",
    "y_test_multi = to_categorical(y_test_multi, num_classes=len(entity_index))\n",
    "\n",
    "print('y_train labels: {}'.format(len(y_train_multi)))\n",
    "print('y_val labels: {}'.format(len(y_val_multi)))\n",
    "print('y_test labels: {}'.format(len(y_test_multi)))\n",
    "np.save(\"Dataset/GRU/{0}_reidx/y_train_multi{0}_reidx.npy\".format(N_TRAININGPOINTS), y_train_multi)\n",
    "np.save(\"Dataset/GRU/{0}_reidx/y_val_multi{0}_reidx.npy\".format(N_TRAININGPOINTS), y_val_multi)\n",
    "np.save(\"Dataset/GRU/{0}_reidx/y_test_multi{0}_reidx.npy\".format(N_TRAININGPOINTS), y_test_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"Dataset/GRU/{0}_reidx/word_index{0}_reidx.json\".format(N_TRAININGPOINTS), \"w\") as f:\n",
    "    json.dump(word_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"Dataset/GRU/{0}_reidx/entity_index{0}_reidx.json\".format(N_TRAININGPOINTS), \"w\") as f:\n",
    "    json.dump(entity_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_bi = []\n",
    "for i in range(len(News_train_word)):\n",
    "    y = []\n",
    "    for j in News_train_word[i]:\n",
    "        if j == word_index[trainsets_reidx[\"answers\"][i]]:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "    y_train_bi.append(y)\n",
    "y_train_bi = np.array(y_train_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_val_bi = []\n",
    "for i in range(len(News_val_word)):\n",
    "    y = []\n",
    "    for j in News_val_word[i]:\n",
    "        if j == word_index[valsets_reidx[\"answers\"][i]]:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "    y_val_bi.append(y)\n",
    "y_val_bi = np.array(y_val_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_bi = []\n",
    "for i in range(len(News_test_word)):\n",
    "    y = []\n",
    "    for j in News_test_word[i]:\n",
    "        if j == word_index[testsets_reidx[\"answers\"][i]]:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "    y_test_bi.append(y)\n",
    "y_test_bi = np.array(y_test_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train labels: 8716\n",
      "y_val labels: 3579\n",
      "y_test labels: 2992\n"
     ]
    }
   ],
   "source": [
    "print('y_train labels: {}'.format(len(y_train_bi)))\n",
    "print('y_val labels: {}'.format(len(y_val_bi)))\n",
    "print('y_test labels: {}'.format(len(y_test_bi)))\n",
    "np.save(\"Dataset/GRU/{0}_reidx/y_train_bi{0}_reidx.npy\".format(N_TRAININGPOINTS), y_train_bi)\n",
    "np.save(\"Dataset/GRU/{0}_reidx/y_val_bi{0}_reidx.npy\".format(N_TRAININGPOINTS), y_val_bi)\n",
    "np.save(\"Dataset/GRU/{0}_reidx/y_test_bi{0}_reidx.npy\".format(N_TRAININGPOINTS), y_test_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc = []\n",
    "for i in range(len(News_train_word)):\n",
    "    cc.append(Counter(News_train_word[i])[word_index[trainsets_reidx[\"answers\"][i]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index_reverse = {code:i for i,code in word_index.items()}\n",
    "entity_index_word = [code for i, code in word_index.items() if i.startswith('@entity') or i == UNK_ENTITY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load1000\n",
      "load2000\n",
      "load3000\n",
      "load4000\n",
      "load5000\n",
      "load6000\n",
      "load7000\n",
      "load8000\n",
      "load9000\n",
      "load10000\n",
      "load11000\n",
      "load12000\n",
      "load13000\n",
      "load14000\n",
      "load15000\n",
      "load16000\n",
      "load17000\n",
      "load18000\n",
      "load19000\n",
      "load20000\n",
      "load21000\n",
      "load22000\n",
      "load23000\n",
      "load24000\n",
      "load25000\n",
      "load26000\n",
      "load27000\n",
      "load28000\n",
      "load29000\n",
      "load30000\n",
      "load31000\n",
      "load32000\n",
      "load33000\n",
      "load34000\n",
      "load35000\n",
      "load36000\n",
      "load37000\n",
      "load38000\n",
      "load39000\n",
      "load40000\n",
      "load41000\n",
      "load42000\n",
      "load43000\n",
      "load44000\n",
      "load45000\n",
      "load46000\n",
      "load47000\n",
      "load48000\n",
      "load49000\n",
      "load50000\n",
      "load51000\n",
      "load52000\n",
      "load53000\n",
      "load54000\n",
      "load55000\n",
      "load56000\n",
      "load57000\n",
      "load58000\n",
      "load59000\n",
      "load60000\n",
      "load61000\n",
      "load62000\n",
      "load63000\n",
      "load64000\n",
      "load65000\n",
      "load66000\n",
      "load67000\n",
      "load68000\n",
      "load69000\n",
      "load70000\n",
      "load71000\n",
      "load72000\n",
      "load73000\n",
      "load74000\n",
      "load75000\n",
      "load76000\n",
      "load77000\n",
      "load78000\n",
      "load79000\n",
      "load80000\n",
      "load81000\n",
      "load82000\n",
      "load83000\n",
      "load84000\n",
      "load85000\n",
      "load86000\n",
      "load87000\n"
     ]
    }
   ],
   "source": [
    "option_input = np.zeros((len(N_train), len(entity_index)))\n",
    "count=0\n",
    "for record in N_train:\n",
    "    for code in record:\n",
    "        if code in entity_index_word:\n",
    "            word = word_index_reverse[code]\n",
    "            entity = entity_index[word]\n",
    "            option_input[count][entity] = code\n",
    "    count+=1\n",
    "    if count%1000 == 0:\n",
    "        print('load' + str(count))\n",
    "np.save(\"Dataset/GRU/100000_reidx/option_input_train100000_reidx.npy\", option_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([34., 14., 18., 22., 25.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "option_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.random.rand(*option_input.shape) * option_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.31804787, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.58736229, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.0796733 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.72333583, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.82288651, 0.        , 0.        ,\n",
       "       0.07012083, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        ])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22697368421052633"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "207/len(valsets_reidx[\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
